{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capital-berkeley",
   "metadata": {},
   "source": [
    "# Legacy Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "functioning-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import LayoutLMForSequenceClassification, LayoutLMTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytesseract\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D\n",
    "import numpy as np\n",
    "\n",
    "classes = [\"bill\", \"invoice\", \"others\", \"Purchase_Order\", \"remittance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-cedar",
   "metadata": {},
   "source": [
    "# Legacy Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "german-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def normalize_box(box, width, height):\n",
    "     return [\n",
    "         int(1000 * (box[0] / width)),\n",
    "         int(1000 * (box[1] / height)),\n",
    "         int(1000 * (box[2] / width)),\n",
    "         int(1000 * (box[3] / height)),\n",
    "     ]\n",
    "\n",
    "def apply_ocr(example):\n",
    "        # get the image\n",
    "        image = Image.open(example['image_path'])\n",
    "\n",
    "        width, height = image.size\n",
    "        \n",
    "        # apply ocr to the image \n",
    "        ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
    "        float_cols = ocr_df.select_dtypes('float').columns\n",
    "        ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "        ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
    "        ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "\n",
    "        # get the words and actual (unnormalized) bounding boxes\n",
    "        #words = [word for word in ocr_df.text if str(word) != 'nan'])\n",
    "        words = list(ocr_df.text)\n",
    "        words = [str(w) for w in words]\n",
    "        coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
    "        actual_boxes = []\n",
    "        for idx, row in coordinates.iterrows():\n",
    "            x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
    "            actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+width, top+height) to get the actual box \n",
    "            actual_boxes.append(actual_box)\n",
    "        \n",
    "        # normalize the bounding boxes\n",
    "        boxes = []\n",
    "        for box in actual_boxes:\n",
    "            boxes.append(normalize_box(box, width, height))\n",
    "        \n",
    "        # add as extra columns \n",
    "        assert len(words) == len(boxes)\n",
    "        example['words'] = words\n",
    "        example['bbox'] = boxes\n",
    "        return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mathematical-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "\n",
    "def encode_example(example, max_seq_length=512, pad_token_box=[0, 0, 0, 0]):\n",
    "  words = example['words']\n",
    "  normalized_word_boxes = example['bbox']\n",
    "\n",
    "  assert len(words) == len(normalized_word_boxes)\n",
    "\n",
    "  token_boxes = []\n",
    "  for word, box in zip(words, normalized_word_boxes):\n",
    "      word_tokens = tokenizer.tokenize(word)\n",
    "      token_boxes.extend([box] * len(word_tokens))\n",
    "  \n",
    "  # Truncation of token_boxes\n",
    "  special_tokens_count = 2 \n",
    "  if len(token_boxes) > max_seq_length - special_tokens_count:\n",
    "      token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n",
    "  \n",
    "  # add bounding boxes of cls + sep tokens\n",
    "  token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
    "  \n",
    "  encoding = tokenizer(' '.join(words), padding='max_length', truncation=True)\n",
    "  # Padding of token_boxes up the bounding boxes to the sequence length.\n",
    "  input_ids = tokenizer(' '.join(words), truncation=True)[\"input_ids\"]\n",
    "  padding_length = max_seq_length - len(input_ids)\n",
    "  token_boxes += [pad_token_box] * padding_length\n",
    "  encoding['bbox'] = token_boxes\n",
    "\n",
    "  assert len(encoding['input_ids']) == max_seq_length\n",
    "  assert len(encoding['attention_mask']) == max_seq_length\n",
    "  assert len(encoding['token_type_ids']) == max_seq_length\n",
    "  assert len(encoding['bbox']) == max_seq_length\n",
    "\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afraid-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to define the features ourselves as the bbox of LayoutLM are an extra feature\n",
    "features = Features({\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'image_path': Value(dtype='string'),\n",
    "    'words': Sequence(feature=Value(dtype='string')),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-legend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intense-recall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMForSequenceClassification(\n",
       "  (layoutlm): LayoutLMModel(\n",
       "    (embeddings): LayoutLMEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (x_position_embeddings): Embedding(1024, 768)\n",
       "      (y_position_embeddings): Embedding(1024, 768)\n",
       "      (h_position_embeddings): Embedding(1024, 768)\n",
       "      (w_position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LayoutLMEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LayoutLMPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LayoutLMForSequenceClassification.from_pretrained(\"saved_model/run2\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-credits",
   "metadata": {},
   "source": [
    "# Data Processing Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "involved-cycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data [] ['audacious.jpg', 'Developer-564x804.png']\n",
      "['audacious.jpg', 'Developer-564x804.png']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/audacious.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image_path\n",
       "0  test_data/audacious.jpg"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "dataset_path = 'test_data'\n",
    "\n",
    "for label_folder, _, file_names in os.walk(dataset_path):\n",
    "    print(label_folder, _, file_names)\n",
    "    print(file_names)\n",
    "    relative_image_names = []\n",
    "    relative_image_names.append(dataset_path + \"/\" + file_names[0])\n",
    "    images.extend(relative_image_names)\n",
    "test_data = pd.DataFrame.from_dict({'image_path': images})\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "auburn-letter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aba325525c4a6b86718dc6a7dd6fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "updated_test_dataset = test_dataset.map(apply_ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "together-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n",
      "['Essay', 'Writing', 'in', '10', 'Words', '1.', 'Brief', 'Take', 'time', 'to', 'fully', 'understand', 'the', 'brief', 'you', 'have', 'been', 'given.', 'Read', 'it', 'several', 'times', 'and', 'put', 'it', 'into', 'your', 'own', 'words', 'before', 'starting.', 'Ensure', 'your', 'essay', 'states', 'the', 'brief', 'in', 'your', 'introduction,', 'addresses', 'it', 'in', 'your', 'main', 'body', 'and', 'answers', 'it', 'in', 'your', 'conclusion.', 'This', 'will', 'enable', 'your', 'reader', 'to', 'know', 'exactly', 'what', 'your', 'essay', 'is', 'about,', 'how', 'you', 'have', 'addressed', 'it', 'and', 'what', 'you', 'have', 'concluded', 'on', 'this', 'subject.', 'If', 'your', 'brief', 'has', 'several', 'parts', 'to', 'it,', 'divide', 'your', 'word', 'count', 'between', 'them', 'according', 'to', 'their', 'relative', 'importance', 'and', 'structure', 'your', 'main', 'body', 'so', 'that', 'you', 'address', 'each', 'part', 'in', 'turn.', '2.', 'Evidence', 'Collect', 'evidence', 'on', 'your', 'subject.', 'Every', 'claim', 'you', 'make', 'in', 'your', 'essay', 'should', 'be', 'backed', 'up', 'by', 'evidence.', 'Never', 'air', 'your', 'beliefs', 'or', 'opinions', 'without', 'evidence', 'unless', 'specifically', 'asked', 'to', 'in', 'your', 'brief.', 'Ensure', 'the', 'quality', 'and', 'relevance', 'of', 'your', 'evidence', 'is', 'as', 'high', 'as', 'possible.', 'Organise', 'your', 'evidence', 'conceptually', '(e.g.', 'use', 'a', 'mind', 'map).', 'Collect', 'about', 'twice', 'as', 'much', 'evidence', 'as', 'you', 'are', 'planning', 'to', 'use.', 'Prioritise', 'the', 'importance', 'of', 'your', 'evidence,', '.g.', 'the', 'top', '10%,', 'the', 'next', '40%,', 'and', 'the', 'other', '50%.', 'Spend', 'about', 'the', 'same', 'number', 'of', 'words', 'discussing', 'the', 'top', '10%', 'as', 'the', 'next', '40%', 'but', \"don't\", 'refer', 'to', 'the', 'other', '50%.', 'Where', 'possible,', 'cite', 'your', 'top', '10%', 'in', 'more', 'depth', 'in', 'one', 'place', 'rather', 'than', 'using', 'several', 'short', 'citations', 'so', 'that', 'your', 'reader', 'can', 'clearly', 'see', 'you', 'are', 'prioritising', 'this', 'evidence.', 'Avoid', 'regurgitating', 'evidence', 'by', 'evaluating', 'it:', \"don't\", 'ignore', 'conflicting', 'evidence', 'and', 'come', 'to', 'a', 'personal', 'view', 'based', 'on', 'the', 'evidence', 'with', 'an', 'appropriate', 'degree', 'of', 'confidence', 'for', 'each', 'area', 'you', 'cover.', '3.', 'Paragraph', 'Write', 'your', 'essay', 'In', 'paragraphs.', 'Separate', 'each', 'paragraph', 'with', 'a', 'blank', 'line.', 'The', 'average', 'paragraph', 'length', 'should', 'be', 'about', '125', 'words', 'and', '4', 'or', 'more', 'sentences.', 'Paragraphs', 'are', 'the', 'basic', 'building', 'blocks', 'of', 'essays.', 'If', 'you', 'can', 'write', 'good', 'paragraphs', 'you', 'are', 'more', 'than', 'half', 'way', 'to', 'becoming', 'a', 'good', 'essay', 'writer.', 'Short', 'paragraphs', 'indicate', 'ideas', 'which', 'are', 'not', 'fully', 'developed.', '4.', 'Topic', 'Each', 'of', 'your', 'paragraphs', 'should', 'introduce,', 'develop', 'and', 'conclude', 'a', 'single', 'topic.', 'Your', 'reader', 'should', 'clearly', 'be', 'able', 'to', 'understand', 'what', 'each', 'paragraph', 'is', 'about', 'by', 'reading', 'the', 'first', 'one', 'o', 'two', 'sentences.', 'If', 'not,', 'then', 'divide', 'your', 'paragraphs', 'up', 'to', 'avoid', 'rambling.', 'Successive', 'paragraph', 'topics', 'should', 'flow', 'in', 'a', 'logical', 'sequence.', '5.', 'Point', 'Each', 'paragraph', 'should', 'make', 'one', 'main', 'point', '(claim', 'backed', 'up', 'by', 'evidence).', 'When', 'introducing', 'and', 'describing', 'evidence', 'your', 'points', 'should', 'generally', 'come', 'at', 'the', 'beginning', 'of', 'your', 'paragraphs', 'but', 'when', 'discussing', 'ideas', 'in', 'more', 'depth', 'they', 'should', 'generally', 'come', 'at', 'the', 'end.', 'Your', 'points', 'are', 'very', 'important', 'as', 'they', 'are', 'the', 'essence', 'of', 'what', 'you', 'are', 'trying', 'to', 'say', 'to', 'your', 'reader.', '6.', 'Introduction', 'Start', 'your', 'essay', 'with', 'an', 'introduction.', 'Use', 'about', '10%', 'of', 'your', 'word', 'count.', 'Your', 'introduction', 'should', 'contain', 'three', 'stages:', '«', 'Territory', '—', 'establish', 'the', 'wider', 'context', 'of', 'your', 'essay', 'subject', 'by', 'making', 'some', 'simple,', 'generally', 'accepted', 'observations', 'which', 'a', 'non-specialist', 'reader', 'can', 'understand.', 'For', 'longer', 'essays,', 'then', 'start', 'to', 'focus', 'towards', 'the', 'essay', 'subject.', '«', 'Niche', '—', 'explain', 'what', 'your', 'essay', 'is', 'about', 'and', 'why', 'it', 'is', 'important/interesting', '«', 'Occupy', '-', 'explain', 'how', 'you', 'are', 'going', 'to', 'address', 'the', 'subject', 'of', 'your', 'essay', 'in', 'your', 'main', 'body', '(this', 'stage', 'also', 'signals', 'to', 'your', 'reader', 'that', 'your', 'introduction', 'is', 'ending', 'and', 'your', 'main', 'body', 'is', 'about', 'to', 'start).', 'This', 'is', 'saying', 'what', 'you', 'are', 'going', 'to', 'say.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(updated_test_dataset)\n",
    "print(len(df[\"words\"][0]))\n",
    "print(df[\"words\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjusted-magnitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c21942666604cccb4ec65905867fdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_test_dataset = updated_test_dataset.map(lambda example: encode_example(example), \n",
    "                                      features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fallen-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_dataset.set_format(type='torch', columns=['input_ids', 'bbox', 'attention_mask', 'token_type_ids'])\n",
    "test_dataloader = torch.utils.data.DataLoader(encoded_test_dataset, batch_size=1, shuffle=True)\n",
    "test_batch = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accessory-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.4451, -2.6868,  9.6513, -2.4914, -2.8206]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "input_ids = test_batch[\"input_ids\"].to(device)\n",
    "bbox = test_batch[\"bbox\"].to(device)\n",
    "attention_mask = test_batch[\"attention_mask\"].to(device)\n",
    "token_type_ids = test_batch[\"token_type_ids\"].to(device)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, \n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "# prediction = int(torch.max(outputs.data, 1)[1].numpy())\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "documentary-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill: 0%\n",
      "invoice: 0%\n",
      "others: 100%\n",
      "Purchase_Order: 0%\n",
      "remittance: 0%\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "# pt_predictions = F.softmax(outputs[0], dim=-1)\n",
    "# pt_predictions\n",
    "\n",
    "classification_logits = outputs.logits\n",
    "classification_results = torch.softmax(classification_logits, dim=1).tolist()[0]\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(classification_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unsigned-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bill': '0%', 'invoice': '0%', 'others': '100%', 'Purchase_Order': '0%', 'remittance': '0%'}\n"
     ]
    }
   ],
   "source": [
    "thisdict ={}\n",
    "for i in range(len(classes)):\n",
    "    thisdict[classes[i]] = str(int(round(classification_results[i] * 100))) + \"%\"\n",
    "print(thisdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "color-bones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5799e-06, 4.3818e-06, 9.9998e-01, 5.3273e-06, 3.8329e-06]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "pt_predictions = F.softmax(outputs[0], dim=-1)\n",
    "pt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "helpful-seventh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NATIVE T5\n",
    "\n",
    "# generated_answer = model.generate(input_ids, attention_mask=attention_mask, \n",
    "#                                  max_length=decoder_max_len, top_p=0.98, top_k=50)\n",
    "# decoded_answer = tokenizer.decode(generated_answer.numpy()[0])\n",
    "# print(\"Answer: \", decoded_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
